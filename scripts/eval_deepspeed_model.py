#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
DeepSpeedæ¨¡å‹è¯„ä¼°è„šæœ¬
ä¸“é—¨ç”¨äºå¤„ç†DeepSpeed ZeROè®­ç»ƒåçš„æ¨¡å‹ï¼Œåˆå¹¶æƒé‡å¹¶ç”Ÿæˆæµ‹è¯•ç»“æœ
"""

import os
import sys
import json
import torch
import argparse
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.data.dataset_loader import DatasetLoader
from src.utils.helpers import set_seed, detect_dataset_name
from src.config.dataset_config import get_num_labels


def merge_deepspeed_checkpoint(checkpoint_dir, output_dir):
    """åˆå¹¶DeepSpeed ZeROæ£€æŸ¥ç‚¹"""
    print(f"æ­£åœ¨åˆå¹¶DeepSpeedæ£€æŸ¥ç‚¹: {checkpoint_dir}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰zero_to_fp32.pyè„šæœ¬ï¼ˆåœ¨æ ¹ç›®å½•ï¼‰
    checkpoint_root = os.path.dirname(checkpoint_dir)
    zero_to_fp32_script = os.path.join(checkpoint_root, "zero_to_fp32.py")
    if not os.path.exists(zero_to_fp32_script):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°zero_to_fp32.pyè„šæœ¬: {zero_to_fp32_script}")
        return False
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)
    
    # è¿è¡Œåˆå¹¶è„šæœ¬
    import subprocess
    try:
        # ä½¿ç”¨ç»å¯¹è·¯å¾„
        cmd = [
            sys.executable, 
            os.path.abspath(zero_to_fp32_script), 
            os.path.abspath(checkpoint_dir), 
            os.path.abspath(os.path.join(output_dir, "pytorch_model.bin"))
        ]
        
        print(f"æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
        result = subprocess.run(cmd, capture_output=True, text=True, cwd=os.path.dirname(zero_to_fp32_script))
        
        if result.returncode == 0:
            print("âœ… DeepSpeedæ£€æŸ¥ç‚¹åˆå¹¶æˆåŠŸ")
            return True
        else:
            print(f"âŒ åˆå¹¶å¤±è´¥: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"âŒ æ‰§è¡Œåˆå¹¶è„šæœ¬æ—¶å‡ºé”™: {e}")
        return False


def find_latest_checkpoint(checkpoint_dir):
    """æ‰¾åˆ°æœ€æ–°çš„æ£€æŸ¥ç‚¹ç›®å½•"""
    print(f"æœç´¢æ£€æŸ¥ç‚¹ç›®å½•: {checkpoint_dir}")
    checkpoints = []
    
    for item in os.listdir(checkpoint_dir):
        print(f"æ£€æŸ¥é¡¹ç›®: {item}")
        if item.startswith("global_step"):
            try:
                # æå–æ­¥éª¤æ•°å­—ï¼Œæ ¼å¼æ˜¯ "global_step291"
                step_str = item.replace("global_step", "")
                step = int(step_str)
                checkpoints.append((step, item))
                print(f"æ‰¾åˆ°æ£€æŸ¥ç‚¹: {item} (æ­¥éª¤ {step})")
            except Exception as e:
                print(f"è§£ææ£€æŸ¥ç‚¹å¤±è´¥: {item}, é”™è¯¯: {e}")
                continue
    
    if not checkpoints:
        print("æ²¡æœ‰æ‰¾åˆ°æ£€æŸ¥ç‚¹")
        return None
    
    # æŒ‰æ­¥éª¤æ’åºï¼Œè¿”å›æœ€æ–°çš„
    checkpoints.sort(key=lambda x: x[0])
    latest_step, latest_dir = checkpoints[-1]
    
    print(f"æ‰¾åˆ°æœ€æ–°æ£€æŸ¥ç‚¹: {latest_dir} (æ­¥éª¤ {latest_step})")
    return os.path.join(checkpoint_dir, latest_dir)


def load_model_and_tokenizer(model_path, base_model_path, num_labels):
    """åŠ è½½æ¨¡å‹å’Œtokenizer"""
    from transformers import AutoTokenizer, AutoModelForSequenceClassification
    
    print(f"åŠ è½½tokenizer: {model_path}")
    tokenizer = AutoTokenizer.from_pretrained(
        model_path,
        trust_remote_code=True
    )
    
    # ä¸ºLlamaæ¨¡å‹è®¾ç½®padding token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    print(f"åŠ è½½æ¨¡å‹: {model_path}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é…ç½®æ–‡ä»¶
    config_path = os.path.join(model_path, "config.json")
    if not os.path.exists(config_path):
        print(f"âš ï¸ æ¨¡å‹ç›®å½•ç¼ºå°‘config.jsonï¼Œä½¿ç”¨åŸºç¡€æ¨¡å‹é…ç½®")
        # ä½¿ç”¨åŸºç¡€æ¨¡å‹é…ç½®
        model = AutoModelForSequenceClassification.from_pretrained(
            base_model_path,
            num_labels=num_labels,
            trust_remote_code=True
        )
        # åŠ è½½è®­ç»ƒå¥½çš„æƒé‡
        state_dict = torch.load(os.path.join(model_path, "pytorch_model.bin"), map_location="cpu")
        model.load_state_dict(state_dict, strict=False)
        print("âœ… ä½¿ç”¨åŸºç¡€æ¨¡å‹é…ç½®åŠ è½½æˆåŠŸ")
    else:
        # ä½¿ç”¨æ¨¡å‹ç›®å½•çš„é…ç½®
        model = AutoModelForSequenceClassification.from_pretrained(
            model_path,
            num_labels=num_labels,
            trust_remote_code=True
        )
        print("âœ… ä½¿ç”¨æ¨¡å‹ç›®å½•é…ç½®åŠ è½½æˆåŠŸ")
    
    return model, tokenizer


def evaluate_model(model, tokenizer, dataset_loader, output_dir, model_path=None):
    """è¯„ä¼°æ¨¡å‹æ€§èƒ½"""
    print("å¼€å§‹æ¨¡å‹è¯„ä¼°...")
    
    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()
    
    # åŠ è½½æµ‹è¯•é›†
    try:
        raw_dataset = dataset_loader._load_raw_dataset()
        if 'test' not in raw_dataset:
            print("âŒ æµ‹è¯•é›†ä¸å­˜åœ¨")
            return None
        
        test_dataset = raw_dataset['test']
        print(f"âœ… åŠ è½½æµ‹è¯•é›†: {len(test_dataset)} æ ·æœ¬")
    except Exception as e:
        print(f"âŒ åŠ è½½æµ‹è¯•é›†å¤±è´¥: {e}")
        return None
    
    # å¤„ç†æµ‹è¯•é›†
    test_data = []
    for example in test_dataset:
        # ä½¿ç”¨dataset_loaderçš„add_promptæ–¹æ³•
        if hasattr(dataset_loader, 'add_prompt'):
            text = dataset_loader.add_prompt(example['text'])
        else:
            text = example['text']
        
        # ç¼–ç æ–‡æœ¬
        inputs = tokenizer(
            text,
            truncation=True,
            padding=True,
            max_length=512,
            return_tensors="pt"
        )
        
        test_data.append({
            'input_ids': inputs['input_ids'].squeeze(),
            'attention_mask': inputs['attention_mask'].squeeze(),
            'labels': example['label'],
            'text': text,
            'original_text': example['text']
        })
    
    print(f"âœ… å¤„ç†æµ‹è¯•é›†: {len(test_data)} æ ·æœ¬")
    
    # è¿›è¡Œé¢„æµ‹
    from tqdm import tqdm
    from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support, confusion_matrix
    
    predictions = []
    probabilities = []
    true_labels = []
    
    with torch.no_grad():
        for data in tqdm(test_data, desc="é¢„æµ‹ä¸­"):
            input_ids = data['input_ids'].unsqueeze(0).to(device)
            attention_mask = data['attention_mask'].unsqueeze(0).to(device)
            
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            
            # è·å–é¢„æµ‹ç±»åˆ«
            pred = torch.argmax(logits, dim=1).cpu().numpy()[0]
            predictions.append(pred)
            
            # è·å–æ¦‚ç‡åˆ†å¸ƒ
            probs = torch.softmax(logits, dim=1).cpu().numpy()[0]
            probabilities.append(probs.tolist())
            
            # çœŸå®æ ‡ç­¾
            true_labels.append(data['labels'])
    
    # è®¡ç®—æŒ‡æ ‡
    accuracy = accuracy_score(true_labels, predictions)
    precision, recall, f1, support = precision_recall_fscore_support(
        true_labels, predictions, average='weighted'
    )
    cm = confusion_matrix(true_labels, predictions)
    report = classification_report(true_labels, predictions, output_dict=True)
    
    # ç”Ÿæˆè¯¦ç»†ç»“æœ
    detailed_results = []
    for i, (data, pred, prob, true_label) in enumerate(zip(test_data, predictions, probabilities, true_labels)):
        result = {
            'sample_id': i,
            'original_text': data['original_text'],
            'processed_text': data['text'],
            'true_label': int(true_label),
            'predicted_label': int(pred),
            'prediction_correct': int(pred == true_label),
            'probabilities': prob,
            'confidence': float(max(prob)),
            'predicted_class_probability': float(prob[pred])
        }
        detailed_results.append(result)
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ä¿å­˜è¯¦ç»†é¢„æµ‹ç»“æœ
    predictions_file = os.path.join(output_dir, f"test_predictions_{timestamp}.json")
    with open(predictions_file, 'w', encoding='utf-8') as f:
        json.dump(detailed_results, f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜è¯„ä¼°æŒ‡æ ‡
    metrics = {
        'accuracy': float(accuracy),
        'precision': float(precision),
        'recall': float(recall),
        'f1_score': float(f1),
        'classification_report': report,
        'confusion_matrix': cm.tolist(),
        'support': support.tolist() if support is not None else []
    }
    
    metrics_file = os.path.join(output_dir, f"test_metrics_{timestamp}.json")
    with open(metrics_file, 'w', encoding='utf-8') as f:
        json.dump(metrics, f, indent=2, ensure_ascii=False)
    
    # ä¿å­˜æ‘˜è¦æŠ¥å‘Š
    summary_file = os.path.join(output_dir, f"test_summary_{timestamp}.txt")
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write("DeepSpeedæ¨¡å‹æµ‹è¯•é›†è¯„ä¼°ç»“æœ\n")
        f.write("=" * 50 + "\n\n")
        f.write(f"æ¨¡å‹è·¯å¾„: {model_path}\n")
        f.write(f"æ—¶é—´æˆ³: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æ€»æ ·æœ¬æ•°: {len(detailed_results)}\n\n")
        
        f.write("æ€§èƒ½æŒ‡æ ‡:\n")
        f.write(f"  å‡†ç¡®ç‡: {metrics['accuracy']:.4f}\n")
        f.write(f"  ç²¾ç¡®ç‡: {metrics['precision']:.4f}\n")
        f.write(f"  å¬å›ç‡: {metrics['recall']:.4f}\n")
        f.write(f"  F1åˆ†æ•°: {metrics['f1_score']:.4f}\n")
        
        f.write(f"\nåˆ†ç±»æŠ¥å‘Š:\n")
        f.write(f"{report}\n")
        
        f.write(f"\næ··æ·†çŸ©é˜µ:\n")
        f.write(f"{cm}\n")
    
    print(f"\nâœ… è¯„ä¼°å®Œæˆï¼ç»“æœå·²ä¿å­˜åˆ°:")
    print(f"  - é¢„æµ‹ç»“æœ: {predictions_file}")
    print(f"  - è¯„ä¼°æŒ‡æ ‡: {metrics_file}")
    print(f"  - æ‘˜è¦æŠ¥å‘Š: {summary_file}")
    
    # æ˜¾ç¤ºä¸»è¦æŒ‡æ ‡
    print(f"\nğŸ“Š æ€§èƒ½æ‘˜è¦:")
    print(f"  å‡†ç¡®ç‡: {accuracy:.4f}")
    print(f"  ç²¾ç¡®ç‡: {precision:.4f}")
    print(f"  å¬å›ç‡: {recall:.4f}")
    print(f"  F1åˆ†æ•°: {f1:.4f}")
    
    return detailed_results, metrics


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="DeepSpeedæ¨¡å‹è¯„ä¼°è„šæœ¬")
    
    # å¿…éœ€å‚æ•°
    parser.add_argument("--checkpoint_dir", type=str, required=True,
                       help="DeepSpeedæ£€æŸ¥ç‚¹ç›®å½•")
    parser.add_argument("--data_path", type=str, required=True,
                       help="æ•°æ®é›†è·¯å¾„")
    parser.add_argument("--output_dir", type=str, required=True,
                       help="è¾“å‡ºç›®å½•")
    
    # å¯é€‰å‚æ•°
    parser.add_argument("--base_model_path", type=str, default=None,
                       help="åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆç”¨äºLoRAæ¨¡å‹ï¼‰")
    parser.add_argument("--seed", type=int, default=42,
                       help="éšæœºç§å­")
    parser.add_argument("--model_max_length", type=int, default=512,
                       help="æ¨¡å‹æœ€å¤§è¾“å…¥é•¿åº¦")
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    set_seed(args.seed)
    print(f"éšæœºç§å­è®¾ç½®ä¸º: {args.seed}")
    
    # æ£€æµ‹æ•°æ®é›†ç±»å‹
    dataset_name = detect_dataset_name(args.data_path)
    if dataset_name:
        num_labels = get_num_labels(dataset_name)
        print(f"æ£€æµ‹åˆ°æ•°æ®é›†: {dataset_name}ï¼Œæ ‡ç­¾æ•°: {num_labels}")
    else:
        num_labels = 7
        print(f"æ— æ³•æ£€æµ‹æ•°æ®é›†ç±»å‹ï¼Œä½¿ç”¨é»˜è®¤æ ‡ç­¾æ•°: {num_labels}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # æ‰¾åˆ°æœ€æ–°æ£€æŸ¥ç‚¹
    latest_checkpoint = find_latest_checkpoint(args.checkpoint_dir)
    if not latest_checkpoint:
        print("âŒ æ‰¾ä¸åˆ°æ£€æŸ¥ç‚¹ç›®å½•")
        return 1
    
    # æ£€æŸ¥æ¨¡å‹ç±»å‹å¹¶é€‰æ‹©åˆé€‚çš„åŠ è½½æ–¹å¼
    pytorch_model_file = os.path.join(args.checkpoint_dir, "pytorch_model.bin")
    
    if os.path.exists(pytorch_model_file):
        print("âœ… æ£€æµ‹åˆ°æ ‡å‡†PyTorchæ¨¡å‹æ–‡ä»¶ï¼Œç›´æ¥ä½¿ç”¨")
        merged_model_dir = args.checkpoint_dir
    else:
        print("ğŸ”„ æ£€æµ‹åˆ°DeepSpeedæ£€æŸ¥ç‚¹ï¼Œéœ€è¦åˆå¹¶æƒé‡")
        merged_model_dir = os.path.join(args.output_dir, "merged_model")
        
        # åœ¨åˆå¹¶ä¹‹å‰ï¼Œå¤åˆ¶latestæ–‡ä»¶åˆ°æ£€æŸ¥ç‚¹å­ç›®å½•
        latest_file = os.path.join(args.checkpoint_dir, "latest")
        if os.path.exists(latest_file):
            import shutil
            latest_dst = os.path.join(latest_checkpoint, "latest")
            shutil.copy2(latest_file, latest_dst)
            print(f"âœ… å¤åˆ¶latestæ–‡ä»¶åˆ°æ£€æŸ¥ç‚¹ç›®å½•: {latest_dst}")
        
        if not merge_deepspeed_checkpoint(latest_checkpoint, merged_model_dir):
            print("âŒ åˆå¹¶æ£€æŸ¥ç‚¹å¤±è´¥")
            return 1
        
        # å¤åˆ¶é…ç½®æ–‡ä»¶åˆ°åˆå¹¶åçš„æ¨¡å‹ç›®å½•
        import shutil
        config_files = ['config.json', 'tokenizer.json', 'tokenizer_config.json', 'special_tokens_map.json']
        for file in config_files:
            src_file = os.path.join(args.checkpoint_dir, file)
            dst_file = os.path.join(merged_model_dir, file)
            if os.path.exists(src_file):
                shutil.copy2(src_file, dst_file)
                print(f"âœ… å¤åˆ¶é…ç½®æ–‡ä»¶: {file}")
    
    # åˆ›å»ºè®­ç»ƒå‚æ•°ï¼ˆç”¨äºdataset_loaderï¼‰
    from transformers import TrainingArguments
    training_args = TrainingArguments(
        output_dir=args.output_dir,
        dataloader_pin_memory=False,
        seed=args.seed
    )
    
    # åˆ›å»ºæ•°æ®å‚æ•°
    from src.config.training_args import DataArguments
    data_args = DataArguments(
        data_path=args.data_path
    )
    
    # åˆ›å»ºdataset_loader
    dataset_loader = DatasetLoader(data_args, training_args, dataset_name)
    
    # åŠ è½½æ¨¡å‹å’Œtokenizer
    model, tokenizer = load_model_and_tokenizer(merged_model_dir, args.base_model_path, num_labels)
    
    # è¯„ä¼°æ¨¡å‹
    results = evaluate_model(model, tokenizer, dataset_loader, args.output_dir, merged_model_dir)
    
    if results:
        print("\nğŸ‰ è¯„ä¼°æˆåŠŸå®Œæˆï¼")
        return 0
    else:
        print("\nâŒ è¯„ä¼°å¤±è´¥")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)
